# 부하 테스트 결과

## 테스트 목적

1. **수평 확장 효과 검증** - WAS 인스턴스 증가 시 처리량 향상 확인
2. **Stateless 아키텍처 검증** - 분산 환경에서 부하가 균등 분산되는지 확인

---

## 테스트 환경

| 항목 | 값 |
|------|-----|
| PostgreSQL | 17 (제한 없음) |
| Redis | 7 |
| Nginx | round_robin |
| WAS | Spring Boot (CPU 0.3, RAM 256M 각각) |
| 좌석 수 | 10000개 |
| 테스트 도구 | k6 |

### 자원 제한 설정

로컬 환경에서 대용량 트래픽을 직접 생성할 수 없으므로, Docker 자원을 제한하여 고부하 상황을 시뮬레이션한다.

| 컨테이너 | CPU | Memory | 목적 |
|----------|-----|--------|------|
| WAS (각각) | 0.3 | 256M | 수평 확장 효과 측정 |

---

## 수평 확장 효과 검증

### 테스트 방식

WAS 1대 vs 3대를 **별도 테스트**로 진행하여 처리량을 비교한다.

**공통 설정**
- VUs: 500
- max-concurrent: 500 (전원 동시 접속)
- HikariCP: 10
- 테스트 시간: 70초 (10s ramp-up + 50s 유지 + 10s ramp-down)

---

### 테스트 A-1: WAS 1대

**k6 결과**

| 지표 | 값 |
|------|-----|
| http_reqs | 14.5/s |
| p95 응답시간 | 34.9s |
| 에러율 | 19.44% |
| 예매 성공률 | 0% |

**스크린샷**

k6 터미널 결과:
<img width="964" height="750" alt="스크린샷 2026-02-10 오후 4 51 34" src="https://github.com/user-attachments/assets/4d51da56-9faf-4260-9712-720d67457788" />

Grafana:
<img width="1470" height="625" alt="스크린샷 2026-02-10 오후 4 50 25" src="https://github.com/user-attachments/assets/87cba78b-6a16-4eed-a7f9-bb2e65533bde" />

**분석**
- WAS 1대 (CPU 0.3)로는 500 VUs 감당 불가
- 서버 과부하로 대부분 요청 타임아웃/EOF
- Grafana 메트릭 수집도 실패하여 그래프 끊김

---

### 테스트 A-2: WAS 3대

**k6 결과**

| 지표 | 값 |
|------|-----|
| http_reqs | 91.1/s |
| p95 응답시간 | 11.67s |
| 에러율 | 6.29% |
| 예매 성공률 | 35.37% (231/653) |

**스크린샷**

k6 터미널 결과:
<img width="1083" height="766" alt="스크린샷 2026-02-10 오후 5 07 26" src="https://github.com/user-attachments/assets/b0f636ce-068b-4994-bedf-50a22a40fd9b" />

Grafana:
<img width="1469" height="618" alt="스크린샷 2026-02-10 오후 5 06 59" src="https://github.com/user-attachments/assets/7255e78e-08c6-4801-b11d-7f11f85ef676" />

**분석**
- WAS 3대 (CPU 0.9)로 500 VUs 처리 가능
- 3대가 균등하게 부하 분산 (각 ~50 RPS)
- 서버 다운 없이 안정적으로 처리
- 예매 231건 성공
- **HikariCP Pending 발생**: app1에서 최대 ~20, app2에서 최대 ~10의 Pending 관찰 → 커넥션 풀 병목 존재

---

### 수평 확장 효과 비교

| 지표 | WAS 1대 | WAS 3대 | 변화 |
|------|---------|---------|------|
| http_reqs | 14.5/s | 91.1/s | **+528%** |
| p95 응답시간 | 34.9s | 11.67s | **-67%** |
| 에러율 | 19.44% | 6.29% | **-68%** |
| 예매 성공률 | 0% | 35.37% | **0% → 35%** |

**결론**
- WAS 1대는 500 VUs 트래픽을 감당하지 못해 서버 과부하 발생
- WAS 3대로 수평 확장 시 처리량 6배 증가, 응답시간 67% 감소
- Stateless 설계로 부하가 균등 분산됨을 확인
- **수평 확장이 대용량 트래픽 처리에 효과적임을 검증**

---

## Overselling 방지 검증

부하 테스트 후 DB에서 중복 예매 여부를 확인했다:

```sql
SELECT seat_id, COUNT(*)
FROM reservations
GROUP BY seat_id
HAVING COUNT(*) > 1;
```

**결과: 0건** - 동일 좌석에 대한 중복 예매 없음.

분산 락(Redisson) + DB 락(SELECT FOR UPDATE) 이중 방어로 Overselling이 방지됨을 확인했다.

---

## 커넥션 풀 설계

### 병목 현상 관찰

WAS 3대 테스트에서 HikariCP를 확인해보니 Pending이 발생했다:

| 인스턴스 | Active (최대) | Pending (최대) |
|----------|---------------|----------------|
| app1 | 10 (풀 전체) | ~20 |
| app2 | 10 (풀 전체) | ~10 |
| app3 | 10 (풀 전체) | ~5 |

커넥션 풀이 포화 상태가 되면서 대기 요청이 발생했다.

### 측정의 한계

실제로 postgresql의 max_connections인 100 기준으로 인스턴스가 3개이니 기존 커넥션 풀 10개 -> 30개로 변경하여 테스트를 진행해보았다.
하지만 인스턴스 별로 균등한 요청이 가지 않아 pending 현상 해소가 되지 않았다.

이유를 생각해보면 이 테스트는 e2e 시나리오(대기열 → 토큰 → 예매)로 진행했기 때문에 HikariCP 병목을 정확히 검증하기 어렵다:

```
대기열 진입 → 상태 폴링 → 토큰 발급 → 예매
 (Redis)   (Redis)   (Redis)  (DB)
```

- 전체 RPS는 균등 분산되지만, **예매 요청**은 유저가 토큰을 받는 타이밍에 따라 특정 WAS에 몰릴 수 있다
- HikariCP는 예매 요청(DB 사용)에서만 측정되므로 불균등하게 나타날 수 있다
- 커넥션 풀을 정확히 테스트하려면 예매 API만 직접 호출하는 별도 테스트가 필요하다

아래 내용은 이러한 한계를 감안한 **설계 지침** 수준으로 참고.

### 커넥션 풀 크기

커넥션 풀이 너무 적으면 Pending이 발생하고, 너무 많으면 DB의 max_connections를 초과할 수 있다.

### 오토 스케일링 환경

고정 인스턴스 환경에서는 단순 계산으로 충분하지만, 오토 스케일링 환경에서는 인스턴스 증가에 따른 총 커넥션 수를 예측하기 어렵다.

**해결 방법:**

| 방식 | 설명 |
|------|------|
| PgBouncer | DB 앞단에 커넥션 프록시 배치 |
| RDS Proxy | AWS 매니지드 커넥션 풀링 |

---

## 최종 결론

### 수평 확장

> WAS 1대로 감당할 수 없는 트래픽(500 VUs)을 3대로 확장하여 처리했다.
> 처리량 6배 증가, 응답시간 67% 감소, 예매 성공률 0% → 35%로 개선.
> Stateless 아키텍처 덕분에 세션 공유 없이 즉시 스케일 아웃 가능.

### 커넥션 풀 설계

> 테스트 중 HikariCP Pending 발생을 관찰했다. 다만 e2e 테스트 특성상 정확한 검증은 어려우며, 별도 테스트가 필요하다.
> 오토 스케일링 환경에서는 PgBouncer나 RDS Proxy 같은 커넥션 풀러를 고려할 수 있다.

### 대용량 트래픽 처리 전략

> 1. **대기열 시스템**: Redis 기반으로 순간 트래픽을 흡수
> 2. **수평 확장**: Stateless WAS로 처리량 선형 증가
